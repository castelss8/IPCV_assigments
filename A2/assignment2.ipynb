{"cells":[{"cell_type":"markdown","metadata":{"id":"MNBgGYg_lpVN"},"source":["# Assignment Module 2: Product Classification\n","\n","The goal of this assignment is to implement a neural network that classifies smartphone pictures of products found in grocery stores. The assignment will be divided into two parts: first, you will be asked to implement from scratch your own neural network for image classification; then, you will fine-tune a pretrained network provided by PyTorch.\n"]},{"cell_type":"markdown","metadata":{"id":"dVTQUJ4uYH1w"},"source":["## Preliminaries: the dataset\n","\n","The dataset you will be using contains natural images of products taken with a smartphone camera in different grocery stores:\n","\n","<p align=\"center\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Granny-Smith.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Pink-Lady.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Lemon.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Banana.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Vine-Tomato.jpg\" width=\"150\">\n","</p>\n","<p align=\"center\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Yellow-Onion.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Green-Bell-Pepper.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Arla-Standard-Milk.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Oatly-Natural-Oatghurt.jpg\" width=\"150\">\n","  <img src=\"https://github.com/marcusklasson/GroceryStoreDataset/raw/master/sample_images/natural/Alpro-Fresh-Soy-Milk.jpg\" width=\"150\">\n","</p>\n","\n","The products belong to the following 43 classes:\n","```\n","0.  Apple\n","1.  Avocado\n","2.  Banana\n","3.  Kiwi\n","4.  Lemon\n","5.  Lime\n","6.  Mango\n","7.  Melon\n","8.  Nectarine\n","9.  Orange\n","10. Papaya\n","11. Passion-Fruit\n","12. Peach\n","13. Pear\n","14. Pineapple\n","15. Plum\n","16. Pomegranate\n","17. Red-Grapefruit\n","18. Satsumas\n","19. Juice\n","20. Milk\n","21. Oatghurt\n","22. Oat-Milk\n","23. Sour-Cream\n","24. Sour-Milk\n","25. Soyghurt\n","26. Soy-Milk\n","27. Yoghurt\n","28. Asparagus\n","29. Aubergine\n","30. Cabbage\n","31. Carrots\n","32. Cucumber\n","33. Garlic\n","34. Ginger\n","35. Leek\n","36. Mushroom\n","37. Onion\n","38. Pepper\n","39. Potato\n","40. Red-Beet\n","41. Tomato\n","42. Zucchini\n","```\n","\n","The dataset is split into training (`train`), validation (`val`), and test (`test`) set."]},{"cell_type":"markdown","metadata":{"id":"1pdrmJRnJPd8"},"source":["The following code cells download the dataset and define a `torch.utils.data.Dataset` class to access it. This `Dataset` class will be the starting point of your assignment: use it in your own code and build everything else around it."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"POMX_3x-_bZI"},"outputs":[{"name":"stderr","output_type":"stream","text":["Cloning into 'GroceryStoreDataset'...\n","Updating files:   6% (393/5717)\n","Updating files:   7% (401/5717)\n","Updating files:   8% (458/5717)\n","Updating files:   9% (515/5717)\n","Updating files:  10% (572/5717)\n","Updating files:  11% (629/5717)\n","Updating files:  11% (641/5717)\n","Updating files:  12% (687/5717)\n","Updating files:  13% (744/5717)\n","Updating files:  14% (801/5717)\n","Updating files:  15% (858/5717)\n","Updating files:  15% (903/5717)\n","Updating files:  16% (915/5717)\n","Updating files:  17% (972/5717)\n","Updating files:  18% (1030/5717)\n","Updating files:  19% (1087/5717)\n","Updating files:  19% (1127/5717)\n","Updating files:  20% (1144/5717)\n","Updating files:  21% (1201/5717)\n","Updating files:  21% (1253/5717)\n","Updating files:  22% (1258/5717)\n","Updating files:  23% (1315/5717)\n","Updating files:  24% (1373/5717)\n","Updating files:  24% (1386/5717)\n","Updating files:  25% (1430/5717)\n","Updating files:  26% (1487/5717)\n","Updating files:  27% (1544/5717)\n","Updating files:  27% (1575/5717)\n","Updating files:  28% (1601/5717)\n","Updating files:  29% (1658/5717)\n","Updating files:  30% (1716/5717)\n","Updating files:  31% (1773/5717)\n","Updating files:  31% (1777/5717)\n","Updating files:  32% (1830/5717)\n","Updating files:  33% (1887/5717)\n","Updating files:  34% (1944/5717)\n","Updating files:  34% (1992/5717)\n","Updating files:  35% (2001/5717)\n","Updating files:  36% (2059/5717)\n","Updating files:  37% (2116/5717)\n","Updating files:  38% (2173/5717)\n","Updating files:  38% (2201/5717)\n","Updating files:  39% (2230/5717)\n","Updating files:  40% (2287/5717)\n","Updating files:  41% (2344/5717)\n","Updating files:  42% (2402/5717)\n","Updating files:  42% (2407/5717)\n","Updating files:  43% (2459/5717)\n","Updating files:  44% (2516/5717)\n","Updating files:  45% (2573/5717)\n","Updating files:  45% (2582/5717)\n","Updating files:  46% (2630/5717)\n","Updating files:  47% (2687/5717)\n","Updating files:  48% (2745/5717)\n","Updating files:  48% (2769/5717)\n","Updating files:  49% (2802/5717)\n","Updating files:  50% (2859/5717)\n","Updating files:  51% (2916/5717)\n","Updating files:  52% (2973/5717)\n","Updating files:  52% (2984/5717)\n","Updating files:  53% (3031/5717)\n","Updating files:  54% (3088/5717)\n","Updating files:  55% (3145/5717)\n","Updating files:  55% (3193/5717)\n","Updating files:  56% (3202/5717)\n","Updating files:  57% (3259/5717)\n","Updating files:  58% (3316/5717)\n","Updating files:  59% (3374/5717)\n","Updating files:  59% (3406/5717)\n","Updating files:  60% (3431/5717)\n","Updating files:  61% (3488/5717)\n","Updating files:  62% (3545/5717)\n","Updating files:  63% (3602/5717)\n","Updating files:  63% (3620/5717)\n","Updating files:  64% (3659/5717)\n","Updating files:  65% (3717/5717)\n","Updating files:  66% (3774/5717)\n","Updating files:  66% (3813/5717)\n","Updating files:  67% (3831/5717)\n","Updating files:  68% (3888/5717)\n","Updating files:  69% (3945/5717)\n","Updating files:  70% (4002/5717)\n","Updating files:  70% (4035/5717)\n","Updating files:  71% (4060/5717)\n","Updating files:  72% (4117/5717)\n","Updating files:  73% (4174/5717)\n","Updating files:  74% (4231/5717)\n","Updating files:  74% (4254/5717)\n","Updating files:  75% (4288/5717)\n","Updating files:  76% (4345/5717)\n","Updating files:  77% (4403/5717)\n","Updating files:  78% (4460/5717)\n","Updating files:  78% (4472/5717)\n","Updating files:  79% (4517/5717)\n","Updating files:  80% (4574/5717)\n","Updating files:  81% (4631/5717)\n","Updating files:  82% (4688/5717)\n","Updating files:  82% (4700/5717)\n","Updating files:  83% (4746/5717)\n","Updating files:  84% (4803/5717)\n","Updating files:  85% (4860/5717)\n","Updating files:  86% (4917/5717)\n","Updating files:  86% (4922/5717)\n","Updating files:  87% (4974/5717)\n","Updating files:  88% (5031/5717)\n","Updating files:  89% (5089/5717)\n","Updating files:  89% (5120/5717)\n","Updating files:  90% (5146/5717)\n","Updating files:  91% (5203/5717)\n","Updating files:  92% (5260/5717)\n","Updating files:  93% (5317/5717)\n","Updating files:  93% (5321/5717)\n","Updating files:  94% (5374/5717)\n","Updating files:  95% (5432/5717)\n","Updating files:  96% (5489/5717)\n","Updating files:  96% (5533/5717)\n","Updating files:  97% (5546/5717)\n","Updating files:  98% (5603/5717)\n","Updating files:  99% (5660/5717)\n","Updating files: 100% (5717/5717)\n","Updating files: 100% (5717/5717), done.\n"]}],"source":["!git clone https://github.com/marcusklasson/GroceryStoreDataset.git"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"hiF8xGEYlsu8"},"outputs":[],"source":["import numpy as np\n","from pathlib import Path\n","from PIL import Image, ImageOps, ImageEnhance\n","import cv2\n","from typing import List, Tuple\n","\n","import torch\n","from torch import Tensor\n","from torch.utils.data import Dataset\n","import torchvision\n","from torchvision import transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"jROSO2qVDxdD"},"outputs":[],"source":["class GroceryStoreDataset(Dataset):\n","\n","    def __init__(self, split: str, transform=None) -> None:\n","        super().__init__()\n","\n","        self.root = Path(\"GroceryStoreDataset/dataset\")\n","        self.split = split # used for splitting the dataset in train and test\n","        self.paths, self.labels = self.read_file() #reading the labels and images\n","\n","        self.transform = transform \n","\n","    def __len__(self) -> int: #get the number of elements in the dataset\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx) -> Tuple[Tensor, int]: \n","        img = Image.open(self.root / self.paths[idx])\n","        label = self.labels[idx]\n","\n","        if self.transform:\n","            img = self.transform(img)\n","\n","        return img, label\n","\n","    # legge un file di testo che contiene i percorsi delle immagini e le etichette associate\n","    # restituisce due liste: una con percorsi delle immagini e una con le etichette\n","    def read_file(self) -> Tuple[List[str], List[int]]: #outputs a tuple\n","        paths = []\n","        labels = []\n","\n","        with open(self.root / f\"{self.split}.txt\") as f:\n","            for line in f:\n","                # path, fine-grained class, coarse-grained class\n","                path, _, label = line.replace(\"\\n\", \"\").split(\", \") #rimuove il carattere di nuova linea, e poi divide in 3 parti con virgola come separatore \n","                paths.append(path), labels.append(int(label)) #salva solo percorsi ed etichette\n","\n","        return paths, labels\n","\n","    def get_num_classes(self) -> int:\n","        return max(self.labels) + 1"]},{"cell_type":"markdown","metadata":{"id":"yBch3dpwNSsW"},"source":["## Part 1: design your own network\n","\n","Your goal is to implement a convolutional neural network for image classification and train it on `GroceryStoreDataset`. You should consider yourselves satisfied once you obtain a classification accuracy on the **validation** split of **around 60%**. You are free to achieve that however you want, except for a few rules you must follow:\n","\n","- You **cannot** simply instantiate an off-the-self PyTorch network. Instead, you must construct your network as a composition of existing PyTorch layers. In more concrete terms, you can use e.g. `torch.nn.Linear`, but you **cannot** use e.g. `torchvision.models.alexnet`.\n","\n","- Justify every *design choice* you make. Design choices include network architecture, training hyperparameters, and, possibly, dataset preprocessing steps. You can either (i) start from the simplest convolutional network you can think of and add complexity one step at a time, while showing how each step gets you closer to the target ~60%, or (ii) start from a model that is already able to achieve the desired accuracy and show how, by removing some of its components, its performance drops (i.e. an *ablation study*). You can *show* your results/improvements however you want: training plots, console-printed values or tables, or whatever else your heart desires: the clearer, the better.\n","\n","Don't be too concerned with your network performance: the ~60% is just to give you an idea of when to stop. Keep in mind that a thoroughly justified model with lower accuracy will be rewarded **more** points than a poorly experimentally validated model with higher accuracy."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["train_dataset = GroceryStoreDataset(split=\"train\", transform=None)\n","test_dataset = GroceryStoreDataset(split=\"test\", transform=None)\n","val_dataset = GroceryStoreDataset(split=\"val\", transform=None)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of elements in the train dataset: 2640\n","Number of elements in the test dataset: 2485\n","Number of elements in the val dataset: 296\n"]}],"source":["train_elements = len(train_dataset)\n","print(f\"Number of elements in the train dataset: {train_elements}\")\n","\n","test_elements = len(test_dataset)\n","print(f\"Number of elements in the test dataset: {test_elements}\")\n","\n","val_elements = len(val_dataset)\n","print(f\"Number of elements in the val dataset: {val_elements}\")"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Label: 29\n"]}],"source":["img, label = train_dataset[2040]\n","print(f\"Label: {label}\")\n","img.show()  # Mostra l'immagine"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Images shape:  torch.Size([3, 348, 348])\n"]}],"source":["transform = transforms.ToTensor()\n","img_tensor = transform(img)\n","print(f\"Images shape: \", img_tensor.shape)"]},{"cell_type":"markdown","metadata":{},"source":["Images shape:  torch.Size([3, 348, 348])\n","\n","- 3 canali = immagine a colori (rgb)\n","- 348 pixel di altezza\n","- 348 pixel di larghezza\n","\n","è un'immagine rgb quadrata con risolutione 348x348 pixel"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Num classes in train dataset:  43\n","Num classes in test dataset:  43\n","Num classes in val dataset:  43\n"]}],"source":["train_num_classes = train_dataset.get_num_classes()\n","print(f\"Num classes in train dataset: \", train_num_classes) #43 perchè la prima ha index 0\n","\n","test_num_classes = test_dataset.get_num_classes()\n","print(f\"Num classes in test dataset: \", test_num_classes)\n","\n","val_num_classes = val_dataset.get_num_classes()\n","print(f\"Num classes in val dataset: \", val_num_classes)\n","\n","#ogni dataset ha lo stesso numero di classi, lo split è stato fatto corettamente "]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["First 5 paths: ['train/Fruit/Apple/Golden-Delicious/Golden-Delicious_001.jpg', 'train/Fruit/Apple/Golden-Delicious/Golden-Delicious_002.jpg', 'train/Fruit/Apple/Golden-Delicious/Golden-Delicious_003.jpg', 'train/Fruit/Apple/Golden-Delicious/Golden-Delicious_004.jpg', 'train/Fruit/Apple/Golden-Delicious/Golden-Delicious_005.jpg']\n","First 5 labels: [0, 0, 0, 0, 0]\n"]}],"source":["paths, labels = train_dataset.read_file()\n","\n","# Stampare i primi 5 percorsi e le etichette corrispondenti\n","print(\"First 5 paths:\", paths[:5])\n","print(\"First 5 labels:\", labels[:5])"]},{"cell_type":"markdown","metadata":{},"source":["### possibile funzione trasform \n","Penso questa sia la fare di pre-processing quando si lavora con le immagini\n","si può fare la stessa cosa con torchvision (ma non ho capito se la possiamo utilizzare o no) oppure con PIL, OpenCV, scikit-image, albumentations"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def pil_transform(image):\n","    # Ridimensiona l'immagine\n","    image = image.resize((128, 128))\n","    \n","    # Capovolge l'immagine orizzontalmente\n","    if np.random.rand() > 0.5:\n","        image = ImageOps.mirror(image)\n","    \n","    # Ruota l'immagine di un angolo casuale entro 30 gradi\n","    angle = np.random.uniform(-30, 30)\n","    image = image.rotate(angle)\n","    \n","    # Modifica casualmente la luminosità\n","    enhancer = ImageEnhance.Brightness(image)\n","    image = enhancer.enhance(np.random.uniform(0.5, 1.5))\n","    \n","    # Converte l'immagine in un array numpy e poi in un tensor di PyTorch\n","    image = np.array(image)\n","    image = torch.tensor(image).permute(2, 0, 1).float() / 255.0\n","    \n","    # Normalizza l'immagine\n","    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n","    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n","    image = (image - mean) / std\n","    \n","    return image"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def opencv_transform(image):\n","    # Convertire l'immagine PIL in un array numpy\n","    image = np.array(image)\n","    \n","    # Ridimensiona l'immagine\n","    image = cv2.resize(image, (128, 128))\n","    \n","    # Capovolge l'immagine orizzontalmente\n","    if np.random.rand() > 0.5:\n","        image = cv2.flip(image, 1)\n","    \n","    # Ruota l'immagine di un angolo casuale entro 30 gradi\n","    angle = np.random.uniform(-30, 30)\n","    M = cv2.getRotationMatrix2D((64, 64), angle, 1)\n","    image = cv2.warpAffine(image, M, (128, 128))\n","    \n","    # Modifica casualmente la luminosità\n","    alpha = np.random.uniform(0.5, 1.5)\n","    image = cv2.convertScaleAbs(image, alpha=alpha, beta=0)\n","    \n","    # Converte l'immagine in un tensor di PyTorch\n","    image = torch.tensor(image).permute(2, 0, 1).float() / 255.0\n","    \n","    # Normalizza l'immagine\n","    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n","    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n","    image = (image - mean) / std\n","    \n","    return image"]},{"cell_type":"markdown","metadata":{},"source":["# Simple CNN \n","\n","https://www.digitalocean.com/community/tutorials/writing-cnns-from-scratch-in-pytorch\n"]},{"cell_type":"markdown","metadata":{},"source":[" The first layer of a CNN is always a convolutional layer. A convolutional layer applies a filter (or kernel) to the input, and passes the result to the next layer. The filter is usually smaller that the image (most often, we can find 3x3 or 5x5 filters), and it moves across the image from top left to bottom right, detecting different features and simplifying the image before passing it onto the next layer. With each convolutional layer, we have to use an activation function. ReLU is a linear function that became the default activation function for many types of neural networks, because it makes the model easy to train, doesn‘t get the vanishing gradient problem compared to Sigmoid and TanH, and often achieves better results.\n","\n","Conv2D\n","\n","There are many different types of convolutional layers, but 2D Convolution Layer (Conv2D) is the most common one. The filter \"slides\" over the 2D input data and multiplies each pixel. As a result, it sums up everything to a single output pixel.\n","\n","Input shape\n","\n","As input, CNN takes tensors of shape (image_height, image_width, color_channels). For color images, color channels are RGB (red, green, blue). The format of CIFAR images is (32, 32, 3), so that‘s what we will pass to the CNN."]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["class CNN(nn.Module):\n","    def __init__(self, num_classes=43):\n","        super(CNN, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n","        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1)\n","        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        \n","        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n","        self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n","        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        \n","        self.fc1 = nn.Linear(1600, 128) #input di dimensione 1600, 128 dimensione di output \n","        self.relu1 = nn.ReLU()\n","        self.fc2 = nn.Linear(128, num_classes)\n","\n","    def forward(self, x):\n","        out = self.conv_layer1(x)\n","        out = self.conv_layer2(out)\n","        out = self.max_pool1(out)\n","        \n","        out = self.conv_layer3(out)\n","        out = self.conv_layer4(out)\n","        out = self.max_pool2(out)\n","                \n","        out = out.reshape(out.size(0), -1)\n","        \n","        out = self.fc1(out)\n","        out = self.relu1(out)\n","        out = self.fc2(out)\n","        return out"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["model = CNN(num_classes=42)"]},{"cell_type":"markdown","metadata":{},"source":["#### hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#loss function\n","loss_fn = nn.CrossEntropyLoss()\n","\n","#optimizer\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"markdown","metadata":{},"source":["\n","**kernel_size**\n","\n","- specifica la dimensione della finestra del filtro che viene applicata all'immagine durante l'operazione di convoluzione\n","- se kernel_size=3 allora il filtro sarà una matrice 3x3\n","- un kernel grande cattura più contesto dell'immaigne ma aumenta anche il numero di parametri e il costo computazionale \n","\n","**stride**\n","- specifica di quanti pixel il filtro si sposta o striscia (?) sull'immagine durante l'operazione di convoluzione\n","- se stride=1 si sposta di un pixel alla volta, se stride=2 di due pixel alla volta e così via \n","- un stride maggiore riduce la dimensione spaziale dell'output poichè il filtro copre meno posizioni sull'immagine\n","\n","**padding**\n","- aggiunge pixel extra (tipicamente con valore zero) attorno ai bordi dell'immagine prima di applicare la convoluzione\n","- se padding=1 viene aggiunto un bordo di un pixel attorno all'immagine\n","- aiuta a preservare la dimensione spaziale dell'immagine dopo la convoluzione, senza la dimensione dell'immagine si riduce ad ogni convoluzione"]},{"cell_type":"markdown","metadata":{},"source":["**MaxPooling (MaxPool2d)**\n","\n","Il layer di Max Pooling (MaxPool2d in PyTorch) è utilizzato nelle reti neurali convoluzionali (CNN) per ridurre la dimensione spaziale (altezza e larghezza) dell'input (che porta a una riduzione del numero di parametri e del costo computazionale nei layer successivi), mantenendo le caratteristiche più importanti (seleziona il valore massimo in ogni finestra del kernel). Questo processo è noto come \"downsampling\". Utile anche per la riduzione di overfitting. \n","Gli strati di maxpooling tipicamente vengono aggiunti dopo ogni convolutional layer insieme a un layer relu"]},{"cell_type":"markdown","metadata":{"id":"gkWEqSPoUIL3"},"source":["## Part 2: fine-tune an existing network\n","\n","Your goal is to fine-tune a pretrained **ResNet-18** model on `GroceryStoreDataset`. Use the implementation provided by PyTorch, do not implement it yourselves! (i.e. exactly what you **could not** do in the first part of the assignment). Specifically, you must use the PyTorch ResNet-18 model pretrained on ImageNet-1K (V1). Divide your fine-tuning into two parts:\n","\n","1. First, fine-tune the Resnet-18 with the same training hyperparameters you used for your best model in the first part of the assignment.\n","1. Then, tweak the training hyperparameters in order to increase the accuracy on the validation split of `GroceryStoreDataset`. Justify your choices by analyzing the training plots and/or citing sources that guided you in your decisions (papers, blog posts, YouTube videos, or whatever else you find enlightening). You should consider yourselves satisfied once you obtain a classification accuracy on the **validation** split **between 80 and 90%**."]}],"metadata":{"colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":0}
